{"cells":[{"cell_type":"markdown","id":"619bad90","metadata":{"id":"619bad90"},"source":["# Assignment — Knowledge Graph Embedding"]},{"cell_type":"code","execution_count":null,"id":"4a87c585","metadata":{"id":"4a87c585"},"outputs":[],"source":["import requests\n","from tqdm.notebook import tqdm, trange\n","import pandas as pd\n","from zlib import adler32\n","import numpy as np\n","\n","pd.options.display.max_colwidth = 100"]},{"cell_type":"markdown","id":"b949edad","metadata":{"id":"b949edad"},"source":["### Task 1. Freebase dataset (0.0 points)"]},{"cell_type":"markdown","id":"f6478877","metadata":{"id":"f6478877"},"source":["In this assignment we will see how to use the [TorchKGE](https://github.com/torchkge-team/torchkge) library for building knowledge graphs and its embeddings. To begin with we are going to need a knowledge graph, so let us load a standard knowledge graph dataset called _Freebase-15k-237_."]},{"cell_type":"code","execution_count":null,"id":"facb26f6","metadata":{"id":"facb26f6"},"outputs":[],"source":["df = pd.read_csv(\n","    \"https://raw.githubusercontent.com/netspractice/network-science/main/datasets/freebase-237-merged-and-remapped.csv.gz\", \n","    names=['h', 'r', 't'])"]},{"cell_type":"code","execution_count":null,"id":"1d9db44d","metadata":{"id":"1d9db44d"},"outputs":[],"source":["df = df[~df.h.str.startswith('/') & ~df.t.str.startswith('/')]\n","df[::1000].head(10)"]},{"cell_type":"markdown","id":"1c6aaa64","metadata":{"id":"1c6aaa64"},"source":["There is h — head (also subject), r — relation (also predicat, label), t — tail (also object). The shape of the dataset is"]},{"cell_type":"code","execution_count":null,"id":"a6f31676","metadata":{"id":"a6f31676"},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","id":"b3f19608","metadata":{"id":"b3f19608"},"source":["Let us check the number of unique entities and unique relations.\n","\n","Write a funtion `n_ent_rel` that takes a dataset and returns a number of unique entities and unique relations.\n"]},{"cell_type":"code","execution_count":null,"id":"4d3ccfb7","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-a1c37b647b2bd8d4","locked":false,"schema_version":3,"solution":true,"task":false},"id":"4d3ccfb7"},"outputs":[],"source":["def n_ent_rel(df):\n","    ### BEGIN SOLUTION\n","    X = df.values\n","    n_ent = np.unique(np.concatenate([X[:, 0], X[:, 2]])).shape[0]\n","    n_rel = np.unique(X[:, 1]).shape[0]\n","    return n_ent, n_rel\n","    ### END SOLUTION"]},{"cell_type":"code","execution_count":null,"id":"e185c183","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-e297130eb1b840de","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"e185c183"},"outputs":[],"source":["n_ent, n_rel = n_ent_rel(df)\n","assert adler32(str(n_ent + n_ent).encode()) == 50266365\n","n_ent, n_rel"]},{"cell_type":"markdown","id":"cae14cdb","metadata":{"id":"cae14cdb"},"source":["We can look at all facts about any entity via pandas API"]},{"cell_type":"code","execution_count":null,"id":"71697bdd","metadata":{"scrolled":false,"id":"71697bdd"},"outputs":[],"source":["df[df.h ==  'aleksandr pushkin'].head()"]},{"cell_type":"code","execution_count":null,"id":"a9d4f918","metadata":{"id":"a9d4f918"},"outputs":[],"source":["df[(df.h ==  'aleksandr pushkin') & (df.t == 'nikolai gogol')]"]},{"cell_type":"code","execution_count":null,"id":"7fb63604","metadata":{"id":"7fb63604"},"outputs":[],"source":["df[(df.h ==  'nikolai gogol') & (df.t == 'aleksandr pushkin')]"]},{"cell_type":"markdown","id":"dc9bdf03","metadata":{"id":"dc9bdf03"},"source":["Let us try to find some facts in this dataset. For example, what is Harrison Ford's nationality? (\"harrison ford\" in the dataset)\n","\n","Write a function `harrison_ford_nationality` that takes a dataset and returns the nationality.\n","\n","_Hint: use `pandas.Series.str.contains` method_"]},{"cell_type":"code","execution_count":null,"id":"4e6bb702","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-bf2eb40c10e9971d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"4e6bb702"},"outputs":[],"source":["def harrison_ford_nationality(df):\n","    ### BEGIN SOLUTION\n","    return df[(df.h ==  'harrison ford') & df.r.str.contains('nationality')].t.iloc[0]\n","    ### END SOLUTION"]},{"cell_type":"code","execution_count":null,"id":"4320c1f6","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-e20681a97039cdf6","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"4320c1f6"},"outputs":[],"source":["assert adler32(harrison_ford_nationality(df).encode()) == 1948191013\n","harrison_ford_nationality(df)"]},{"cell_type":"markdown","id":"473842ae","metadata":{"id":"473842ae"},"source":["More tricky question: who are film directors of movies where Harrison Ford was?\n","\n","Write a function `made_films_with_harrison_ford` that returns a set of directors' names."]},{"cell_type":"code","execution_count":null,"id":"2f0f48bf","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-7ea1e48135c759d5","locked":false,"schema_version":3,"solution":true,"task":false},"id":"2f0f48bf"},"outputs":[],"source":["def made_films_with_harrison_ford(df):\n","    ### BEGIN SOLUTION\n","    films = df[(df.h ==  'harrison ford') & df.r.str.contains('actor')].t.tolist()\n","    directors = df[df.t.isin(films) & df.r.str.contains('director')].h\n","    return set(directors)\n","    ### END SOLUTION"]},{"cell_type":"code","execution_count":null,"id":"57e619bd","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-f3dbc03094dcbb5b","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"57e619bd"},"outputs":[],"source":["directors = made_films_with_harrison_ford(df)\n","assert adler32(str(sorted(directors)).encode()) == 3798415057\n","directors"]},{"cell_type":"markdown","id":"106f7373","metadata":{"id":"106f7373"},"source":["### Task 2. Translation models (0.0 points)"]},{"cell_type":"markdown","id":"4b1fe0e9","metadata":{"id":"4b1fe0e9"},"source":["TransE (translations in the embedding space) is a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities.\n","\n","Let us implement the model using TorchKGE — a Python module for knowledge graph embedding relying solely on Pytorch."]},{"cell_type":"code","execution_count":null,"id":"d59878ae","metadata":{"id":"d59878ae"},"outputs":[],"source":["# use for installation to colab/locally. Don't use it when upload to evalai\n","\n","# !pip install torchkge==0.16.25 -q"]},{"cell_type":"code","execution_count":null,"id":"ca8a243b","metadata":{"id":"ca8a243b"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchkge import KnowledgeGraph\n","from torchkge.utils import Trainer\n","from torchkge.evaluation import LinkPredictionEvaluator\n","from torchkge.models.interfaces import TranslationModel\n","from torch.nn.init import xavier_uniform_"]},{"cell_type":"markdown","id":"913de2fd","metadata":{"id":"913de2fd"},"source":["<img src='https://github.com/netspractice/ml-on-graphs/raw/main/assignment_knowledge_graph_embedding/kge.png' width=300>"]},{"cell_type":"markdown","id":"d24c0ca2","metadata":{"id":"d24c0ca2"},"source":["The basic idea behind the model is that the functional relation induced by the $r$-labeled edges corresponds to a translation of the embeddings, i.e. we want that $h + r \\approx t$ when ($h$, $r$, $t$) holds ($t$ should be a nearest neighbor of $h + r$), while $h + r$ should be far away from $t$ otherwise. Thereby, TransE scoring function is negative distance between $h+r$ and $t$\n","\n","$$f(h, r, t) = -||h + r - t||$$\n","\n","where $||\\cdot||$ is L1 or L2 norm. The goal of the optimization procedure is learning optimal embeddings, such that the scoring function is able to assign high scores to positive statements and low scores to statements unlikely to be true. The most common loss function is Margin Loss that can be represented as\n","\n","$$L = \\text{ReLU}\\left(\\gamma - f(h, r, t) + f(h', r, t')\\right)$$\n","\n","where $\\gamma > 0$ is a \"margin\" hyperparameter, the triplet $(h, r, t)$ is in a training set (positive) and the $(h', r, t')$ is in a set of *corrupted* (negative) triplets, that are composed of training triplets with either the head or tail replaced by a random entity (but not both at the same time)."]},{"cell_type":"markdown","id":"87808531","metadata":{"id":"87808531"},"source":["We will use TorchKGE TranslationModel as a base for TransE class, here are some auxiliary methods: \n","\n","* `normalize_parameters` applies `L2` normalization for training stability\n","* `get_embeddings` returns the tensors representing entities and relations\n","* `lp_prep_cands` gets entities and relations embeddings for link prediction training process\n","* `forward` returns scores for positive triplets (`h`, `r`, `t`) and negative triplets (`nh`, `r`, `nt`)"]},{"cell_type":"code","execution_count":null,"id":"7280f803","metadata":{"id":"7280f803"},"outputs":[],"source":["class BaseTransE(TranslationModel):\n","    def __init__(self, num_entities, num_relations, dim=100):\n","        super(BaseTransE, self).__init__(num_entities, num_relations, dissimilarity_type='L2')\n","        self.num_entities = num_entities\n","        self.num_relations = num_relations\n","        self.dim = dim\n","        self.ent_embeddings = nn.Embedding(num_entities, self.dim)\n","        self.rel_embeddings = nn.Embedding(num_relations, self.dim)\n","        \n","    def normalize_parameters(self):\n","        self.ent_embeddings.weight.data = F.normalize(self.ent_embeddings.weight.data, p=2, dim=1)\n","\n","    def get_embeddings(self):\n","        self.normalize_parameters()\n","        return self.ent_embeddings.weight.data, self.rel_embeddings.weight.data\n","        \n","    def lp_prep_cands(self, h_idx, t_idx, r_idx):\n","        b_size = h_idx.shape[0]\n","\n","        h_emb = self.ent_embeddings(h_idx)\n","        t_emb = self.ent_embeddings(t_idx)\n","        r_emb = self.rel_embeddings(r_idx)\n","\n","        candidates = self.ent_embeddings.weight.data.view(1, self.num_entities, self.dim)\n","        candidates = candidates.expand(b_size, self.num_entities, self.dim)\n","\n","        return h_emb, t_emb, candidates, r_emb\n","    \n","    def forward(self, h, t, nh, nt, r):\n","        return self.scoring_function(h, t, r), self.scoring_function(nh, nt, r)"]},{"cell_type":"markdown","id":"ca3ee02b","metadata":{"id":"ca3ee02b"},"source":["Write a function `scoring_function` that takes a tensor `h`, `t`, `r` with triplets of the shape (number of triplets) and returns its L2 scores. For training stability, apply L2 normalization (`F.normalize`  with `p=2`) on entities embeddings before computing the scores. Note that `h`, `t`, `r` are tensors with integer IDs of entities and relations."]},{"cell_type":"code","execution_count":null,"id":"d0011822","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-10c450a856899a4f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"d0011822"},"outputs":[],"source":["class TransE(BaseTransE):\n","    def scoring_function(self, h, t, r):\n","        ### BEGIN SOLUTION\n","        h = self.ent_embeddings(h)\n","        t = self.ent_embeddings(t)\n","        r = self.rel_embeddings(r)\n","        h = F.normalize(h, p=2, dim=1)\n","        t = F.normalize(t, p=2, dim=1)\n","        score = (h + r) - t\n","        return - torch.norm(score, p=2, dim=1)\n","        ### END SOLUTION"]},{"cell_type":"code","execution_count":null,"id":"fdd6e41d","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-89f5ccbf84380079","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"fdd6e41d"},"outputs":[],"source":["model = TransE(num_entities=5, num_relations=2, dim=2)\n","\n","h = torch.LongTensor([1, 1, 1])\n","r = torch.LongTensor([0, 1, 1])\n","t = torch.LongTensor([2, 2, 3])\n","\n","with torch.no_grad():\n","    scores = model.scoring_function(h, t, r).numpy()\n","assert scores.shape == (3, ) # 3 triplets"]},{"cell_type":"markdown","id":"1460dae6","metadata":{"id":"1460dae6"},"source":["Complete the class `MarginLoss` so that computes the losses among scores and returns an average loss. Positive and negative scores are tensors of the shape (number of triplets)."]},{"cell_type":"code","execution_count":null,"id":"6996f6b3","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-3fb7cb717cb420cb","locked":false,"schema_version":3,"solution":true,"task":false},"id":"6996f6b3"},"outputs":[],"source":["class MarginLoss(nn.Module):\n","    def __init__(self, margin):\n","        super().__init__()\n","        self.margin = margin\n","    def forward(self, positive_scores, negative_scores):\n","        ### BEGIN SOLUTION\n","        return F.relu(self.margin - positive_scores + negative_scores).mean()\n","        ### END SOLUTION"]},{"cell_type":"code","execution_count":null,"id":"1c789666","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-b28b48d0354bf7be","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"1c789666"},"outputs":[],"source":["loss_fn = MarginLoss(margin=0.5)\n","torch.manual_seed(0)\n","positive_scores = torch.randn(8)\n","negative_scores = torch.randn(8)\n","loss = loss_fn(positive_scores, negative_scores)\n","assert loss.shape == torch.Size([])\n","assert round(loss.item(), 2) == 0.79"]},{"cell_type":"markdown","id":"40e66c15","metadata":{"id":"40e66c15"},"source":["Let us look at the WikiDataSet that presents country-specific subgraphs of Wikidata."]},{"cell_type":"code","execution_count":null,"id":"39f768ae","metadata":{"id":"39f768ae"},"outputs":[],"source":["url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_edges.tsv'\n","open('countries_edges.tsv', 'wb').write(requests.get(url).content)\n","url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_entities.tsv'\n","open('countries_entities.tsv', 'wb').write(requests.get(url).content)\n","url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/countries_relations.tsv'\n","open('countries_relations.tsv', 'wb').write(requests.get(url).content);"]},{"cell_type":"code","execution_count":null,"id":"6430aa5b","metadata":{"scrolled":true,"id":"6430aa5b"},"outputs":[],"source":["edges = pd.read_csv('countries_edges.tsv', sep='\t').values\n","entity_labels = pd.read_csv('countries_entities.tsv', sep='\t', index_col=0).label.values\n","relation_labels = pd.read_csv('countries_relations.tsv', sep='\t', index_col=0).label.values"]},{"cell_type":"code","execution_count":null,"id":"cac5db83","metadata":{"id":"cac5db83"},"outputs":[],"source":["edges_labeled = np.stack([entity_labels[edges[:, 0]], \n","                          entity_labels[edges[:, 1]], \n","                          relation_labels[edges[:, 2]]], axis=1)\n","\n","df = pd.DataFrame(edges_labeled, columns=['h', 't', 'r'])[['h', 'r', 't']]\n","df.head()"]},{"cell_type":"markdown","id":"96e03f08","metadata":{"id":"96e03f08"},"source":["Here are unique relations with the number of triplets"]},{"cell_type":"code","execution_count":null,"id":"1d4ef2bd","metadata":{"id":"1d4ef2bd"},"outputs":[],"source":["df.r.groupby(df.r).count()"]},{"cell_type":"markdown","id":"767c8128","metadata":{"id":"767c8128"},"source":["Convert our dataset into a TorchKGE knowledge graph"]},{"cell_type":"code","execution_count":null,"id":"e315fd6c","metadata":{"id":"e315fd6c"},"outputs":[],"source":["kg = KnowledgeGraph(\n","    pd.DataFrame(edges_labeled, columns=['from', 'to', 'rel']))"]},{"cell_type":"markdown","id":"7cd9ac15","metadata":{"id":"7cd9ac15"},"source":["Split the dataset into train and test set. What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to ensure that all entities are represented in train and test sets by at least one triple.\n","\n"]},{"cell_type":"code","execution_count":null,"id":"86485e1b","metadata":{"id":"86485e1b"},"outputs":[],"source":["kg_train, kg_test = kg.split_kg()"]},{"cell_type":"markdown","id":"45027cae","metadata":{"id":"45027cae"},"source":["Create required objects: model, loss, optimizer and trainer."]},{"cell_type":"code","execution_count":null,"id":"949a0024","metadata":{"id":"949a0024"},"outputs":[],"source":["model = TransE(kg_train.n_ent, kg_train.n_rel, dim=64)\n","criterion = MarginLoss(margin=0.5)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n","\n","trainer = Trainer(\n","    model, criterion, kg_train, n_epochs=500, \n","    batch_size=2048, optimizer=optimizer)"]},{"cell_type":"markdown","id":"ddd51ca4","metadata":{"id":"ddd51ca4"},"source":["During training, positive triplets are selected, negative triplets are generated, margin loss is calculated and then the gradient step is performed."]},{"cell_type":"code","execution_count":null,"id":"26513e9f","metadata":{"id":"26513e9f"},"outputs":[],"source":["trainer.run()"]},{"cell_type":"markdown","id":"b27a7911","metadata":{"id":"b27a7911"},"source":["Let us evaluate our model on the link prediction task"]},{"cell_type":"code","execution_count":null,"id":"ba2026aa","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-e6b0a4e916e8c503","locked":true,"points":0,"schema_version":3,"solution":false,"task":false},"id":"ba2026aa"},"outputs":[],"source":["evaluator = LinkPredictionEvaluator(model, kg_test)\n","evaluator.evaluate(b_size=128)\n","\n","assert evaluator.hit_at_k(k=10)[0] > 0.3\n","assert evaluator.mean_rank()[0] < 70\n","assert evaluator.mrr()[0] > 0.1"]},{"cell_type":"code","execution_count":null,"id":"ba1f7516","metadata":{"id":"ba1f7516"},"outputs":[],"source":["print('Hit@10: {:.4f}'.format(evaluator.hit_at_k(k=10)[0]))\n","print('Mean Rank: {:.4f}'.format(evaluator.mean_rank()[0]))\n","print('MRR: {:.4f}'.format(evaluator.mrr()[0]))"]},{"cell_type":"markdown","id":"4c2ce4e5","metadata":{"id":"4c2ce4e5"},"source":["`Hit@k` indicates how many times in average a true triple was ranked in the top-k.\n","\n","`Mean Rank` is a mean rank of the true entity when replacing alternatively head and tail in any fact of the dataset.\n","\n","`MRR` is an average of mean recovery rank for head and tail replacement."]},{"cell_type":"markdown","id":"a332b2e7","metadata":{"id":"a332b2e7"},"source":["### Task 3. Entity embeddings (2.5 points)"]},{"cell_type":"code","execution_count":null,"id":"a0fc34f6","metadata":{"id":"a0fc34f6"},"outputs":[],"source":["from sklearn.decomposition import TruncatedSVD\n","from sklearn.cluster import k_means\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"5433cf80","metadata":{"id":"5433cf80"},"source":["Similarly to node embedding visualization, we can plot 2D embeddings of entities of knowledge graph.\n","\n","Write a function `ent_embeddgins` that takes the model, transforms entity embeddings into 2-dimenstional space via SVD, then finds 2 clusters via k-means and finally returns a tuple: np.array with embeddings and 0/1 labels of clusters."]},{"cell_type":"code","execution_count":null,"id":"dbc9b605","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-c79123779965d790","locked":false,"schema_version":3,"solution":true,"task":false},"id":"dbc9b605"},"outputs":[],"source":["def ent_embeddgins(model):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"6cbae81a","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-c4d5a66741d71c62","locked":true,"points":2.5,"schema_version":3,"solution":false,"task":false},"id":"6cbae81a"},"outputs":[],"source":["ent_emb, labels = ent_embeddgins(model)\n","assert ent_emb.shape == (1810, 2)\n","assert labels.shape == (1810,)\n","n_label1 = (labels == 1).sum()\n","n_label0 = (labels == 0).sum()\n","assert 0.14 < min(n_label1, n_label0) / max(n_label1, n_label0) < 0.18"]},{"cell_type":"code","execution_count":null,"id":"7fe88cd5","metadata":{"id":"7fe88cd5"},"outputs":[],"source":["plt.figure(figsize=(8, 5))\n","colors = np.array(plt.cm.tab10.colors)\n","plt.scatter(ent_emb[:, 0], ent_emb[:, 1], s=10, c=colors[labels])\n","plt.title('Entity embeddings')\n","plt.show()"]},{"cell_type":"markdown","id":"595b2ba3","metadata":{"id":"595b2ba3"},"source":["Let us look at some randomly chosen entities from each cluster. Here we use `kg.ent2ix` dictionary to get labels of entities."]},{"cell_type":"code","execution_count":null,"id":"6b09c78c","metadata":{"id":"6b09c78c"},"outputs":[],"source":["ent_labels = np.array(list(kg.ent2ix.keys()))\n","print('First cluster: ', ', '.join(\n","    np.random.permutation(ent_labels[labels == 0])[:10]))\n","print()\n","print('Second cluster: ', ', '.join(\n","    np.random.permutation(ent_labels[labels == 1])[:10]))"]},{"cell_type":"markdown","id":"df05ba3b","metadata":{"id":"df05ba3b"},"source":["### Task 4. Nearest neighbors of entities (2.5 points)"]},{"cell_type":"code","execution_count":null,"id":"669adc04","metadata":{"id":"669adc04"},"outputs":[],"source":["from sklearn.neighbors import NearestNeighbors"]},{"cell_type":"markdown","id":"e07bdbf6","metadata":{"id":"e07bdbf6"},"source":["Let us find a nearest neighbors of Belgium using embedding space.\n","\n","Write a function `similar_countries` that takes a name of country, graph and model and returns a list with names of nearest countries. Use `model.get_embeddings()`."]},{"cell_type":"code","execution_count":null,"id":"91d490c5","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-fe878bfa8dc0c817","locked":false,"schema_version":3,"solution":true,"task":false},"id":"91d490c5"},"outputs":[],"source":["def similar_countries(name, kg, model):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"dee374e6","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-0b8b02590a965cc3","locked":true,"points":2.5,"schema_version":3,"solution":false,"task":false},"id":"dee374e6"},"outputs":[],"source":["similar = similar_countries('Belgium', kg, model)\n","assert 'Netherlands' in similar\n","similar"]},{"cell_type":"markdown","id":"a80a415f","metadata":{"id":"a80a415f"},"source":["### Task 5. Relation prediction (2.5 points)"]},{"cell_type":"markdown","id":"8bcd2c29","metadata":{"id":"8bcd2c29"},"source":["Let us consider these facts:\n","1. Belgium shares border with France\n","2. Belgium shares border with Switzerland\n","3. Belgium shares border with Nigeria\n","\n","Only the fact 1 is truth, but it seems that the fact 2 is more likely than 3."]},{"cell_type":"code","execution_count":null,"id":"5ed79917","metadata":{"id":"5ed79917"},"outputs":[],"source":["df[(df.h == 'Belgium') & (df.t == 'France')]"]},{"cell_type":"code","execution_count":null,"id":"52a2002d","metadata":{"id":"52a2002d"},"outputs":[],"source":["df[(df.h == 'Belgium') & (df.t == 'Switzerland')]"]},{"cell_type":"code","execution_count":null,"id":"5c5a6e01","metadata":{"id":"5c5a6e01"},"outputs":[],"source":["df[(df.h == 'Belgium') & (df.t == 'Nigeria')]"]},{"cell_type":"markdown","id":"e2e80719","metadata":{"id":"e2e80719"},"source":["Now we can compare these facts using scoring function of TransE model to check our prior knowledge.\n","\n","Write a function `belgium_facts` that takes a model, a graph and returns 3 values of scoring function for each fact. Use `model.scoring_function` with `torch.no_grad()`."]},{"cell_type":"code","execution_count":null,"id":"6e484ea3","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-5feee5e866b961d0","locked":false,"schema_version":3,"solution":true,"task":false},"id":"6e484ea3"},"outputs":[],"source":["def belgium_facts(model, kg):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"0c73a43b","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-a6b6e4e88ba6569f","locked":true,"points":2.5,"schema_version":3,"solution":false,"task":false},"id":"0c73a43b"},"outputs":[],"source":["scores = belgium_facts(model, kg)\n","assert scores[0] > scores[1] > scores[2]\n","scores"]},{"cell_type":"markdown","id":"8eece48c","metadata":{"id":"8eece48c"},"source":["### Task 6. Tail prediction (2.5 points)"]},{"cell_type":"code","execution_count":null,"id":"1feb4bc5","metadata":{"id":"1feb4bc5"},"outputs":[],"source":["from sklearn.neighbors import NearestNeighbors"]},{"cell_type":"markdown","id":"eb36ce98","metadata":{"id":"eb36ce98"},"source":["Recall that the main idea of TransE is to learn $h + r \\approx t$, so let us find the closest tails for the head + relation pair.\n","\n","Write a function `soviet_tail` that takes a model, graph and finds 5 nearest neighbors for the pair $h$ is \"Soviet Union\" and $r$ is \"founded by\"."]},{"cell_type":"code","execution_count":null,"id":"c6f4633a","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-dbc103d80a938c68","locked":false,"schema_version":3,"solution":true,"task":false},"id":"c6f4633a"},"outputs":[],"source":["def soviet_tail(model, kg):\n","    # YOUR CODE HERE\n","    raise NotImplementedError()"]},{"cell_type":"code","execution_count":null,"id":"d8d5ab86","metadata":{"nbgrader":{"grade":true,"grade_id":"cell-ba34f9e5a695e8ca","locked":true,"points":2.5,"schema_version":3,"solution":false,"task":false},"id":"d8d5ab86"},"outputs":[],"source":["tails = soviet_tail(model, kg)\n","assert 'Russian Soviet Federative Socialist Republic' in tails\n","tails"]},{"cell_type":"code","execution_count":null,"id":"8e455805","metadata":{"id":"8e455805"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}